---
title: "Finding fast growing firms"
author: "Maeva Braeckevelt and Brúnó Helmeczy"
output:
  pdf_document: default
  html_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Executive Summary
This analysis aimed at predicting the fast growing of a firm.


```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
# CLEAR MEMORY
rm(list=ls())


# Import libraries
library(haven)
library(glmnet)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)

library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(partykit)
library(rpart.plot)
library(huxtable)


# set working directory
data_dir <- paste0(getwd(),"/DA3_A2_Bruno_Maeva/")
SourceFunctions <- paste0("https://raw.githubusercontent.com/Maeva2408/DA3_A2_Bruno_Maeva/main/code/functions/")

# load theme and functions
source(paste0(SourceFunctions,"theme_bg.R"))
source(paste0(SourceFunctions,"da_helper_functions.R"))

# Loading and preparing data ----------------------------------------------
url <- "https://raw.githubusercontent.com/Maeva2408/DA3_A2_Bruno_Maeva/main/data/clean/bisnode_firms_clean.rds"
data <- readRDS(url(url, method="libcurl"))

# Define variable sets ----------------------------------------------
# (making sure we use ind2_cat, which is a factor)

firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")

Fin1 <-  c("curr_assets", "curr_liab", "extra_exp", 
           "extra_inc", "extra_profit_loss", "fixed_assets",
           "inc_bef_tax", "intang_assets", "inventories", 
           "liq_assets", "material_exp", "personnel_exp",
           "profit_loss_year", "sales", "share_eq", "subscribed_cap",
           "tang_assets","amort", "EBITDA")

Fin2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
          "profit_loss_year_pl_quad", "share_eq_bs_quad",
          c(grep("*flag_low$", names(data), value = TRUE),
            grep("*flag_high$", names(data), value = TRUE),
            grep("*flag_error$", names(data), value = TRUE),
            grep("*flag_zero$", names(data), value = TRUE)))

Fin3 <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs",
          "share_eq_bs", "subscribed_cap_bs", "intang_assets_bs", "extra_exp_pl",
          "extra_inc_pl", "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
          "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl","working_capital_TO",
          "EBITDA_pl")

Growth <- data %>% select(matches("^d1_")) %>% select(-matches(Fin2))%>% colnames()

HR <- c("ceo_age", "ceo_young","flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")

qualityvars <- c("balsheet_flag", "balsheet_length", "balsheet_notfullyear")

# interactions for logit, LASSO
interactions1 <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*d1_sales_mil_log_mod", "ind2_cat*sales_mil_log",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", "ind2_cat*labor_avg_mod",
                   paste0("ind2_cat*", Growth))

interactions2 <- c("sales_mil_log*age", "sales_mil_log*female",
                   "sales_mil_log*profit_loss_year_pl", "sales_mil_log*foreign_management")

X1 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat")
X2 <- c(X1, "fixed_assets_bs","share_eq_bs","curr_liab_bs ",   "curr_liab_bs_flag_high ", 
        "curr_liab_bs_flag_error",  "age","foreign_management" )

X3 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth)
X4 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth, HR, qualityvars, Fin2)
X5 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth, HR, qualityvars, Fin2, interactions1, interactions2)

# separate datasets -------------------------------------------------------
set.seed(13505)
train_indices <- as.integer(createDataPartition(data$HyperGrowth, p = 0.8, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

```

# Introduction

This analysis serves to predict fast growth of firms between 2012 and 2014. This analysis could help an investor to decide to invest in a certain company or not. The data was originally created by Bisnode, a major European business information company. The original dataset, bisnode-firms, considers companies between 2005 & 2016 in both Manufacturing (Electrical equipment, Motor vehicles, etc) & Services (Accommodation, and Food & Beverage services activities). 

As business context, we consider our client to be a silent investor group as of 2012, who is looking for suitable companies to invest in, with expected returns on their investments in 2 years. As such, we considered companies at least 1 year old, since such companies have all accumulated financial data over multiple years, yet many are young, with higher chances of gaining further market share. We only considered businesses generating annual sales between 100 thousand, & 10 million Euros, boasting sufficiently large sales for the business venture to be considered 'serious', yet small enough for it's size not to be considered a disadvantage.

# Data

## Label engineering

The first step of this analysis was to define fast growth. To do so, we used the compound annual growth rate formula. 

CAGR = (Vfinal/Vbegin)^1/t - 1 


t being the time in years, we defined it as 2. 
It seems logic for us that if an investor took interest in a firm, he will be able only to invest the year later.
Vbegin is the Sales (€) in the year ongoing
Vfinal is the Sales (€) in the year +2

we converted in percentage.
we defined any firm that have a % CAGR of bigger or equal at 30% in two years as a fast growing firm, it means that they growth for 70% in two years.

## Sample design

Our sample is defined as 
- Firms still in business in 2012
- Firms that have sales between 10 000€ and 10 millions €

Our fast growing firms are compound annual growth percentage bigger or equal at 30% in two year

After having design our sample, we end up with 15692 companies and 2220 of them (14%) are fast growing.

```{r, eval=TRUE, echo=FALSE, results='asis'}


```
## Feature engineering
#MAEVa
Now, We need to select our x variables and maybe transform them but before we need to clean them.
- Some of the variable are financial accounts (like inventories, current liabilities, etc), so they can't be negative, thus we decided to replace the negative value to 0.
- We created ratios , easier for interpretation and spotting extreme values
- we used the a method called winsorization 
- We elevated some variable to a quadratic function
- We took the log of sales.

We decided to classify the variable 

* **Firm**, _5 variables_ : Age of firm, squared age, a dummy if newly established, industry categories, location regions for its headquarters, and dummy if located in a big city

* **Financial 1**,  _16 variables_ : Winsorized financial variables : sales, fixed, liquid, current, intangible assets, curret liabilities, inventories, equity shares, subscribed capital, sales revenues, income before tax, extra income, material, personal and extra expenditure, extra profit.

* **Financial 2** : Flags (extrem, low, high, zero - when applicable) and polynomials : quadratic term are created for profit and loss, extra profit and loss, income before tax, and share equity.

* **Financial 3** : % blance sheet, % profite and loss

* **Growth**, _X variable_ : Sales growth is captured by a winsorized growth variable, its quadratic term and flags for extreme low and high values.

* **HR**, _5 variable_ : 5 variables :  For the CEO: Female dummy, winsorized age and flags, flags for missing information; foreign management dummy; labor cost, and flag for missing labor cost information.

* **Data quality**, _3 variables_: Variables related to the data quality of the financial information, flag for a problem, and the length of the year that the balance sheet covers.

We chose to include some interactions as well, that we defined by common knwoledge.

* **Interaction** : Interactions with the sales growth, firm size, and industry

```{r, eval=TRUE, echo=FALSE, results='asis'}

```

# Probability prediction and model selection

We decided to use three different models for the prediction of fast growing firm : Logit, Lasso and Random Forest.
Our 5 Logistic regression models gradually incorporate more & more of the variable groups above, where our final model, similarly to LASSO, incorporates all available variables. Finally, our Random Forest model considers all variable groups except interactions.  

## Logit
```{r, message = F,warning = F, echo = FALSE, eval=TRUE}
# 5 fold cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)

# Train Logit Models ----------------------------------------------

logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)

CV_RMSE_folds <- list()
logit_models <- list()
data$HyperGrowth_f <- factor( data$HyperGrowth_f, levels = c("no_Hyp.Growth","Hyp.Growth"), 
        labels = c("no_Hyp.Growth" = 0,"Hyp.Growth" = 1))
for (model_name in names(logit_model_vars)) {

  features <- logit_model_vars[[model_name]]

  set.seed(13505)
  glm_model <- train(
    formula(paste0("HyperGrowth_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )

  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]

}

#CV_RMSE_folds
#logit_models
```
We started by carrying out a probability prediction by logit. Used 5-fold cross-validation to select the best model.

We created five different models. The predictors of the first two models were handpicked and then we gradually add more categories at each models.

* **X1** : Log of sales in Millions, the square of the Log of sales in Millions , Winsorized value of the change of the log of Sales versus last year, profit and loss by total sales, Industries categories

* **X2** : X1, fixed assets divided by total assets, share equity divided by total assets,current liability divided by total assets, hight flags for current liability divided by total assets, flag error for current liability divided by total assets,age, foreign_management

* **X3** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth

* **X4** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth, _Financial 2_, _HR_, _Data quality_

* **X5** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth , Financial 2, HR, Data quality, _Intercactions_

# variables - tables
# Ratio EBITDA
# Show RMSE
# Choose the model




```{r, eval=TRUE, echo=FALSE, results='asis', warning= FALSE}

```

## LASSO 

```{r, message = F,warning = F, echo = FALSE, eval=TRUE}

# Logit lasso -----------------------------------------------------------
logitvars <- c("sales_mil_log", "sales_mil_log_sq", 
               firm, Fin3, Fin2, Growth, HR, qualityvars, interactions1, interactions2)

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(13505)
  logit_lasso_model <- train(
    formula(paste0("HyperGrowth_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
#write.csv(lasso_coeffs, paste0(output, "lasso_logit_coeffs.csv"))

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]
#CV_RMSE_folds[["LASSO"]]


#logit_models
#CV_RMSE_folds

```
# all variables
# comparing lasso to the best logit


## Random forest
```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
# 5 fold cross-validation
rfvars  <-  X4

train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, # same as probability = TRUE in ranger
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
train_control$verboseIter <- TRUE

tune_grid <- expand.grid(
  .mtry = c( 9, 10, 11),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)

# getModelInfo("ranger")
set.seed(13505)
rf_model_p <- train(
  formula(paste0("HyperGrowth_f ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control
)

#rf_model_p$results

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size
logit_models[["RandForest"]] <- rf_model_p
CV_RMSE_folds[["RandForest"]] <- rf_model_p$resample[,c("Resample", "RMSE")]


```
# variable from our best logit model (take off the interaction)
# models

```{r,  message = F,warning = F, echo = FALSE, eval=TRUE}

# Get average (ie over the folds) RMSE and AUC ------------------------------------

# AUC
# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {
  
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$Hyp.Growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                           "AUC" = unlist(auc))
}

#CV_AUC_folds
#CV_RMSE_folds

# For each model: average RMSE and average AUC for models ----------------------------------
CV_RMSE <- list()
CV_AUC <- list()
for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}
#CV_RMSE
#CV_AUC 

# Model Selection ---------------------------------

# 7 models, (5 logit , 1-1 logit LASSO & RF). For each we have a 5-CV RMSE and AUC.
# Nice Summary Table -------------------

nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)

logit_summary1 <- data.frame("Model" = names(logit_models),
                             "Number of predictors" = unlist(nvars),
                             "CV RMSE" = unlist(CV_RMSE),
                             "CV AUC" = unlist(CV_AUC))

logit_summary1 %>% as_hux()

```

# Classification

```{r,  message = F,warning = F, echo = FALSE, eval=TRUE}
# Introduce loss function ------------------
# relative cost of of a false negative classification (as compared with a false positive classification)
 # False Positive: 
    # We invest but should not have -> 20% to loss capital & did not invest in correct firm = -2.2
 # False Negative: 
    # We dont invest but we should have -> we dont triple our money =
 # Scale it to whole numbers -> 10-11
FP=3  
FN=5 
cost <- FN/FP

# CAGR for Hypergrowth companies -> triple in size in 2 years 
data[data$HyperGrowth == 1,] %>% summarize(AvgCAGR = round(mean(CAGR),2))

# CAGR for Non-Hypergrowth companies -> 
  #   No default -> no loss no gain
  #   Default   -> all money lost - ca. 20% among no hypergrowth firms
data[(data$HyperGrowth == 0) && (data$default == 0),] %>% summarize(AvgCAGR = round(mean(CAGR),2))

prop.table(table(data$default,data$HyperGrowth),2) %>% as_hux()
  # this would mean O * 0.8 - 1 * 0.2 = -0.2
    # we also could have used our money else where -> to triple it by giving it to hypergrowth firm:
    # on the other hand one gains more experience by making mistakes - something we also wanted to incorporate
  # 0 * 0.8 - 1 * 0.2 - 2 +1 = -1.2

# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevelance = sum(data_train$HyperGrowth_f == "Hyp.Growth")/length(data_train$HyperGrowth_f)

# Draw ROC Curve and find optimal threshold with loss function --------------------------
best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  best_tresholds_cv <- list()
  expected_loss_cv <- list()

#  fold <- "Fold5"
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)

    roc_obj <- roc(cv_fold$obs, cv_fold$Hyp.Growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevelance))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$Hyp.Growth)
  }

  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv)[is.finite(unlist(best_tresholds_cv)) == T])
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv)[is.finite(unlist(expected_loss_cv)) == T])

  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]

  }

# Summary results ---------------------------------------------------
summary_results <- data.frame("Model" = names(logit_models),
                              "Number of predictors" = unlist(nvars),
                              "CV RMSE" = unlist(CV_RMSE),
                              "CV AUC" = unlist(CV_AUC),
                              "CV threshold" = unlist(best_tresholds),
                              "CV expected Loss" = unlist(expected_loss))
#names(summary_results) <- NULL
rownames(summary_results) <- NULL


summary_results %>% as_hux()


```


## Model Diagnostics & Ex-Sample Testing
```{r, eval=TRUE, echo=FALSE, results='asis'}
# Pick best model based on average expected loss ----------------------------------

best_logit_with_loss <- logit_models[["X4"]]
best_logit_optimal_treshold <- best_tresholds[["X4"]]

logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"Hyp.Growth"]

# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$HyperGrowth, data_holdout[, "best_logit_with_loss_pred", drop=TRUE])

# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)

expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$HyperGrowth)
#expected_loss_holdout


#hist(data_holdout$best_logit_with_loss_pred)

# Confusion table on holdout with optimal threshold
data_holdout$holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_Hyp.Growth", "Hyp.Growth") %>%
  factor(levels = c("no_Hyp.Growth", "Hyp.Growth"))
cm_object3 <- confusionMatrix(data_holdout$holdout_prediction,data_holdout$HyperGrowth_f)
cm3 <- cm_object3$table

cm3 %>% as_hux()

# Average Growth among invested companies
data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% group_by(HyperGrowth_f) %>% 
  summarize(AvgAnnGrowth = mean(CAGR)) %>% as_hux()
```



```{r, eval=TRUE, echo=FALSE, results='asis'}
# Average Growth among invested companies
data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% group_by(HyperGrowth_f) %>% 
  summarize(AvgAnnGrowth = mean(CAGR)) %>% as_hux()

# Loss
 (21 * 10000 * (1 - 0.29)^2) - 210000  
# Win 
 (21 * 10000 * (1+ 0.85)^2) - 210000

data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% 
  group_by(HyperGrowth_f,ind2) %>% 
  summarize(AvgAnnGrowth = mean(CAGR),
            Nr_Investments = n()) %>% as_hux()

```


# Conclusion / Summary

