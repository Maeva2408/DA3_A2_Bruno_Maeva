---
title: "Finding fast growing firms"
author: "Maeva Braeckevelt and Brúnó Helmeczy"
output:
  pdf_document: default
  html_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Executive Summary
This analysis aimed at predicting the fast growing of a firm.


```{r, include=F, message=FALSE, echo=FALSE, eval=TRUE}
# CLEAR MEMORY
rm(list=ls())

# Import libraries
library(gridExtra)
library(haven)
library(glmnet)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)

library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(partykit)
library(rpart.plot)
library(huxtable)



# set working directory
data_dir <- paste0(getwd(),"/DA3_A2_Bruno_Maeva/")
SourceFunctions <- paste0("https://raw.githubusercontent.com/Maeva2408/DA3_A2_Bruno_Maeva/main/code/functions/")

# load theme and functions
source(paste0(SourceFunctions,"theme_bg.R"))
source(paste0(SourceFunctions,"da_helper_functions.R"))

# Loading and preparing data ----------------------------------------------
url <- "https://raw.githubusercontent.com/Maeva2408/DA3_A2_Bruno_Maeva/main/data/clean/bisnode_firms_clean.rds"
data <- readRDS(url(url, method="libcurl"))

# Define variable sets ----------------------------------------------
# (making sure we use ind2_cat, which is a factor)

firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")

Fin1 <-  c("curr_assets", "curr_liab", "extra_exp", 
           "extra_inc", "extra_profit_loss", "fixed_assets",
           "inc_bef_tax", "intang_assets", "inventories", 
           "liq_assets", "material_exp", "personnel_exp",
           "profit_loss_year", "sales", "share_eq", "subscribed_cap",
           "tang_assets","amort", "EBITDA")

Fin2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
          "profit_loss_year_pl_quad", "share_eq_bs_quad",
          c(grep("*flag_low$", names(data), value = TRUE),
            grep("*flag_high$", names(data), value = TRUE),
            grep("*flag_error$", names(data), value = TRUE),
            grep("*flag_zero$", names(data), value = TRUE)))

Fin3 <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs",
          "share_eq_bs", "subscribed_cap_bs", "intang_assets_bs", "extra_exp_pl",
          "extra_inc_pl", "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
          "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl","working_capital_TO",
          "EBITDA_pl")

Growth <- data %>% select(matches("^d1_")) %>% select(-matches(Fin2))%>% colnames()

HR <- c("ceo_age", "ceo_young","flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")

qualityvars <- c("balsheet_flag", "balsheet_length", "balsheet_notfullyear")

# interactions for logit, LASSO
interactions1 <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*d1_sales_mil_log_mod", "ind2_cat*sales_mil_log",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", "ind2_cat*labor_avg_mod",
                   paste0("ind2_cat*", Growth))

interactions2 <- c("sales_mil_log*age", "sales_mil_log*female",
                   "sales_mil_log*profit_loss_year_pl", "sales_mil_log*foreign_management")

X1 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat")
X2 <- c(X1, "fixed_assets_bs","share_eq_bs","curr_liab_bs ",   "curr_liab_bs_flag_high ", 
        "curr_liab_bs_flag_error",  "age","foreign_management" )

X3 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth)
X4 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth, HR, qualityvars, Fin2)
X5 <- c("sales_mil_log", "sales_mil_log_sq", firm, Fin1, Fin3, Growth, HR, qualityvars, Fin2, interactions1, interactions2)

# separate datasets -------------------------------------------------------
set.seed(13505)
train_indices <- as.integer(createDataPartition(data$HyperGrowth, p = 0.8, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

```

# Introduction

This analysis serves to predict fast growth of firms between 2012 and 2014. This analysis could help an investor to decide to invest in a certain company or not. The data was originally created by Bisnode, a major European business information company. The original dataset, bisnode-firms, considers companies between 2005 & 2016 in both Manufacturing (Electrical equipment, Motor vehicles, etc) & Services (Accommodation, and Food & Beverage services activities). 

As business context, we consider our client to be a silent investor group as of 2012, who is looking for suitable companies to invest in, with expected returns on their investments in 2 years. As such, we considered companies at least 1 year old, since such companies have all accumulated financial data over multiple years, yet many are young, with higher chances of gaining further market share. We only considered businesses generating annual sales between 100 thousand, & 10 million Euros, boasting sufficiently large sales for the business venture to be considered 'serious', yet small enough for it's size not to be considered a disadvantage.

# Data

## Label engineering

The first step of this analysis was to define fast growth. To do so, we used the compound annual growth rate formula. 

CAGR = (Vfinal/Vbegin)^1/t - 1 


* t being the time in years, we defined it as 2. 

* It seems logic for us that if an investor took interest in a firm, he will be able only to invest the year later.

* Vbegin is the Sales (in euro) in the year ongoing

* Vfinal is the Sales (in euro) the year +2

we converted in percentage.
we defined any firm that have a % CAGR of bigger or equal at 25% in two years as a fast growing firm, it means that they growth for at least 60% in two years.

## Sample design

Our sample is defined as 
- Firms still in business in 2012
- Firms that have sales between 10 000 euro and 10 millions euro

Our fast growing firms are compound annual growth percentage bigger or equal at 30% in two year

After having design our sample, we end up with 14896 companies and 2413 of them (16%) are fast growing.

## Feature engineering

Now, We need to select our x variables and maybe transform them but before we need to clean them.
Some of the variable are financial accounts (like inventories, current liabilities, etc), so they can't be negative, thus we decided to replace the negative value to 0.
We created ratios , easier for interpretation and spotting extreme values
We also created new varibales : change between the variable now and two years ago, we did it for the log of Sales difference (in million), inventories, total assets and amortization.
we used the a method called winsorization to avoid extrem value. We used it for the change in sales and in total assets.
We elevated some variable to a quadratic function
We took the log of sales.

We decided to classify the variable 

* **Firm**, _5 variables_ : Age of firm, squared age, a dummy if newly established, industry categories, location regions for its headquarters, and dummy if located in a big city

* **Financial 1**,  _16 variables_ : Winsorized financial variables : sales, fixed, liquid, current, intangible assets, curret liabilities, inventories, equity shares, subscribed capital, sales revenues, income before tax, extra income, material personal and extra expenditure, extra profit, EBDTA, amortization and tangible assets.

* **Financial 2** : Flags (extrem, low, high, zero - when applicable) and polynomials : quadratic term are created for profit and loss, extra profit and loss, income before tax, and share equity.

* **Financial 3** : total assets, fixed assets divided by total assets, liquid assets divided by total assets,current assets divided by total assets, share equity divided by total assets, subcribed capital divided by total assets, intangible assets divided by total assets, extra expense divided by total sales, extra income divided by total sales, extra profit and loss divided by total sales, income before tax divided by total sales, inventories divided by total sales, material personal and extra expenditure divided by total sales, profit and loss divided by total sales, personal expense divided by total sales, working capital divided by total sales, EDITDA divided by total sales.
         
* **Growth**, _X variable_ : Sales growth is captured by a winsorized growth variable, its quadratic term and flags for extreme low and high values.

* **HR**, _5 variable_ : 5 variables :  For the CEO: Female dummy, winsorized age and flags, flags for missing information; foreign management dummy; labor cost, and flag for missing labor cost information.

* **Data variables**, _3 variables_: Variables related to the data quality of the financial information, flag for a problem, and the length of the year that the balance sheet covers.

We chose to include some interactions as well, that we defined by common knwoledge.

* **Interaction** : Interactions with the sales growth, firm size, and industry


# Probability prediction and model selection

We decided to use three different models for the prediction of fast growing firm : Logit, Lasso and Random Forest.
Our 5 Logistic regression models gradually incorporate more & more of the variable described groups above, where our final model, similarly to LASSO, incorporates all available variables. Finally, our Random Forest model considers all variable groups except interactions. We used a 5-fold cross-validation to estimate models and then we selected the best model based on expected loss.
To calculate the expected loss we defined the cost of false negative and false positive errors.

## Define loss function

We first calculated the average CAGR amongst Hyper growth, & non-hyper growth companies. We found that companies labeled as hyper growth, on average have a CAGR of 64%, meaning they triple their size in terms of sales in 2 years. We assumed this implies our investors triple their money as well, giving us the base, true-positive scenario. Our False Negative case is when we do not decide to invest, but should have, implying the investors missed out on tripling the capital they would have invested, i.e. we set our False negative = -2.

The False Positive case is when we invest in a firm, but should not have. To estimate our costs in this case, we calculated average CAGR among non-hyper growth firms whom did not go out of business, & found that on average they stagnated (CAGR = 0.45%). However, ca. 18% of non-hyper growth firms defaulted in the 2 year horizon, implying investors losing their capital. Furthermore, by investing in the wrong firm, investors obviously missed out on the expectation of tripling their money by investinng in the right firms. On the other hand, we believe mistakes breed learning & growth. Thus, we set our false positive error as 20% * -1 (Lossing capital due to default) -2 (not tripling investment) +1 (learning from the experience) = -1.2. After rounding the whole numbers for simplicity, our False Positive Error = 3 & False Negative Error = 5

## Modeling
```{r, message = F,warning = F, echo = FALSE, eval=TRUE}
# 5 fold cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)

# Train Logit Models ----------------------------------------------
logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)

CV_RMSE_folds <- list()
logit_models <- list()
data$HyperGrowth_f <- factor( data$HyperGrowth_f, levels = c("no_Hyp.Growth","Hyp.Growth"), 
        labels = c("no_Hyp.Growth" = 0,"Hyp.Growth" = 1))
for (model_name in names(logit_model_vars)) {

  features <- logit_model_vars[[model_name]]

  set.seed(13505)
  glm_model <- train(
    formula(paste0("HyperGrowth_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )

  logit_models[[model_name]] <- glm_model
# Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]

}

# Logit lasso -----------------------------------------------------------
logitvars <- c("sales_mil_log", "sales_mil_log_sq", 
               firm, Fin3, Fin2, Growth, HR, qualityvars, interactions1, interactions2)

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(13505)
  logit_lasso_model <- train(
    formula(paste0("HyperGrowth_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]

# RandForest 5 fold cross-validation
rfvars  <-  X4

train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, # same as probability = TRUE in ranger
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
train_control$verboseIter <- FALSE

tune_grid <- expand.grid(
  .mtry = c( 9, 10, 11),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)

# getModelInfo("ranger")
set.seed(13505)
rf_model_p <- train(
  formula(paste0("HyperGrowth_f ~ ", paste0(rfvars , collapse = " + "))),
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control
)

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size
logit_models[["RandForest"]] <- rf_model_p
CV_RMSE_folds[["RandForest"]] <- rf_model_p$resample[,c("Resample", "RMSE")]


```

The predictors of the first two models were handpicked and then we gradually add more categories at each models.

* **X1** : Log of sales in Millions, the square of the Log of sales in Millions , Winsorized value of the change of the log of Sales versus last year, profit and loss by total sales, Industries categories

* **X2** : X1, fixed assets divided by total assets, share equity divided by total assets,current liability divided by total assets, hight flags for current liability divided by total assets, flag error for current liability divided by total assets,age, foreign_management

* **X3** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth

* **X4** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth, _Financial 2_, _HR_, _Data quality_

* **X5** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth , Financial 2, HR, Data quality, _Interactions_

* **Lasso** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth , Financial 2, HR, Data quality, Interactions

* **Random Forest** : Log of sales in Millions, the square of the Log of sales in Millions, Firm, Financial 1, Growth , Financial 2, HR, Data quality


```{r loss function, message = F,warning = F, echo = FALSE, eval=TRUE}
# Introduce loss function ------------------
# relative cost of of a false negative classification (as compared with a false positive classification)
 # False Positive: 
    # We invest but should not have -> 20% to loss capital & did not invest in correct firm = -2.2
 # False Negative: 
    # We dont invest but we should have -> we dont triple our money =
 # Scale it to whole numbers -> 10-11
FP <- 3  
FN <- 5 
cost <- FN/FP

# CAGR for Hypergrowth companies -> triple in size in 2 years 
data[data$HyperGrowth == 1,] %>% summarize(AvgCAGR = round(mean(CAGR),2))

# CAGR for Non-Hypergrowth companies -> 
  #   No default -> no loss no gain
  #   Default   -> all money lost - ca. 20% among no hypergrowth firms
data[(data$HyperGrowth == 0) && (data$default == 0),] %>% summarize(AvgCAGR = round(mean(CAGR),2))

prop.table(table(data$default,data$HyperGrowth),2) %>% as_hux()
  # this would mean O * 0.8 - 1 * 0.2 = -0.2
    # we also could have used our money else where -> to triple it by giving it to hypergrowth firm:
    # on the other hand one gains more experience by making mistakes - something we also wanted to incorporate
  # 0 * 0.8 - 1 * 0.2 - 2 +1 = -1.2

# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevelance = sum(data_train$HyperGrowth_f == "Hyp.Growth")/length(data_train$HyperGrowth_f)


```


```{r, message = F,warning = F, echo = FALSE, eval=TRUE}
# Get average (ie over the folds) RMSE and AUC ------------------------------------

# AUC
# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {
  
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <- model$pred %>% filter(Resample == fold)
    roc_obj <- roc(cv_fold$obs, cv_fold$Hyp.Growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc), "AUC" = unlist(auc))
}

# For each model: average RMSE and average AUC for models ----------------------------------
CV_RMSE <- list()
CV_AUC <- list()
for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

# Model Selection ---------------------------------

# 7 models, (5 logit , 1-1 logit LASSO & RF). For each we have a 5-CV RMSE and AUC.
# Nice Summary Table -------------------
nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)

best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  best_tresholds_cv <- list()
  expected_loss_cv <- list()

  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <- model$pred %>% filter(Resample == fold)
    roc_obj <- roc(cv_fold$obs, cv_fold$Hyp.Growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevelance))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$Hyp.Growth)
  }
  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv)[is.finite(unlist(best_tresholds_cv)) == T])
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv)[is.finite(unlist(expected_loss_cv)) == T])

  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]
  }

# Summary results ---------------------------------------------------
summary_results <- data.frame("Model" = names(logit_models),
                              "Number of predictors" = unlist(nvars),
                              "CV RMSE" = unlist(CV_RMSE),
                              "CV AUC" = unlist(CV_AUC),
                              "CV threshold" = unlist(best_tresholds),
                              "CV expected Loss" = unlist(expected_loss))
rownames(summary_results) <- NULL
summary_results %>% as_hux()

#grid.arrange(summary_results, ncol = 1, as.table = T)


```


Please see all models summary statistics above. Observed the lowest Root Mean Square Error is 36.1 %, given by models X3, Lasso & Random forest. However all models were very close, the highest RMSE being 36.6%. The highest 'Area Under the Curve' is given by the Random Forest model, with AUC = 65,1%. I.e. Using this model, we are 15% better off than by simple random guessing, & 7% better off vs the base model, X1. However, like for the RMSE, the model X4 is very close to the Random forest with a AUC of 64,8%.


EXPECTED LOSS
We base our final model selection on minimized expected loss, for which we first selected optimal classification threshold values of each model, thereby comparing the best possible expected loss for each model. The smaller expected loss is 0.793 is the model X4. Considering that it is a very theoretical concept, we only interpret it in relative terms: it is the lowest of all models, though our models again show little variation. Thus, our final model choice is Model X4. It had the lowest expected loss (the most important criteria), while the AUC & RMSE were very close to being the best.


## Ex-Sample Testing & Model Diagnostics 
```{r, eval=TRUE, message = F, warning = F, echo=FALSE, results='asis'}
# Pick best model based on average expected loss ----------------------------------

best_logit_with_loss <- logit_models[["X4"]]
best_logit_optimal_treshold <- best_tresholds[["X4"]]

logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"Hyp.Growth"]

# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$HyperGrowth, data_holdout[, "best_logit_with_loss_pred", drop=TRUE])

# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$HyperGrowth)

# Confusion table on holdout with optimal threshold
data_holdout$holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_Hyp.Growth", "Hyp.Growth") %>%
  factor(levels = c("no_Hyp.Growth", "Hyp.Growth"))
cm_object3 <- confusionMatrix(data_holdout$holdout_prediction,data_holdout$HyperGrowth_f)
cm3 <- cm_object3$table

# Benchmark Model --------------------
benchmark_model <- logit_models[["X1"]]
benchmark_threshold <- best_tresholds[["X1"]]

benchmark_predicted_probabilities_holdout <- predict(benchmark_model, newdata = data_holdout, type = "prob")
data_holdout[,"benchmark_pred"] <- benchmark_predicted_probabilities_holdout[,"Hyp.Growth"]

data_holdout$holdout_prediction_benchmark <-
  ifelse(data_holdout$benchmark_pred < benchmark_threshold, "no_Hyp.Growth", "Hyp.Growth") %>%
  factor(levels = c("no_Hyp.Growth", "Hyp.Growth"))
cm_object_benchmark <- confusionMatrix(data_holdout$holdout_prediction_benchmark,data_holdout$HyperGrowth_f)
cm4 <- cm_object_benchmark$table

colnames(cm3) <- c("Final_Model","Final_Model")
colnames(cm4) <- c("Benchmark_Model","Benchmark_Model")
prediction <- c("no_Hyp.Growth","Hyp.Growth")

rbind(prediction,cbind(cm3,cm4)) %>% kable()

# Average Growth among invested companies
data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% group_by(HyperGrowth_f) %>% 
  summarize(AvgAnnGrowth = mean(CAGR)) %>% as_hux()
```



```{r, eval=TRUE, echo=FALSE, results='asis'}
# Average Growth among invested companies
data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% group_by(HyperGrowth_f) %>% 
  summarize(AvgAnnGrowth = mean(CAGR),Firms_Selected = n()) %>% as_hux()


# Loss
 (21 * 10000 * (1 - 0.29)^2) - 210000  
# Win 
 (21 * 10000 * (1+ 0.85)^2) - 210000

data_holdout[data_holdout$holdout_prediction == "Hyp.Growth",] %>% 
  group_by(HyperGrowth_f,ind2) %>% 
  summarize(AvgAnnGrowth = mean(CAGR),
            Nr_Investments = n()) %>% as_hux()

```

# Conclusion / Summary

